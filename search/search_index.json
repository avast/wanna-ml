{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to WANNA","text":"<p>WANNA is a ML platform product that integrates security, data governance and deployment best practices with well-defined constructs provided by GCP Vertex AI native services to make research move faster, more efficiently and predictively from ML prototypes to production models whilst leaving behind the limitations of an on-prem infrastructure.</p>"},{"location":"#objective","title":"Objective","text":"<p>Embrace the advantages and scalability of a ML Platform as a Service and adopt Vertex AI premise of \u201cone AI platform, every ML tool you need\u201d.</p> <p>Having the goal for a unified set of APIs, frameworks, UIs and secure data access patterns from a central point will increase productivity across the Research and Data Science teams whilst removing operational costs.</p>"},{"location":"#proposal","title":"Proposal","text":"<p>Vertex AI native services are vast and welcomed, however we must have clear communication, interfaces, requirements and prioritization to cover all researchers and ML Engineers use cases within this ecosystem. </p> <p>WANNA will provide this thought its CLI, standardized project structure, CI/CD integration and addressing and supporting customer needs.  </p>"},{"location":"#problem-definition","title":"Problem definition","text":"<p>Remove barriers and standardize the management of ML projects on Vertex AI through the lenses of unified tooling.</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#environment-variables","title":"Environment variables","text":"<p>The library uses environment variables to configure the behavior. The following variables are available:</p>"},{"location":"configuration/#offline-runs","title":"Offline runs","text":"<p>This section is related to partial or full offline mode without actually calling GCP services.</p> <ul> <li><code>WANNA_GCP_ACCESS_ALLOWED</code> allows communication with GCP. </li> <li>Default true.</li> <li>Disable for running tests without access to the internet and off-line validations or dry-runs.</li> <li><code>WANNA_GCP_ENABLE_REMOTE_VALIDATION</code> allows validation of GCP resources like region, machine type, etc.</li> <li>Default true.</li> <li>Disable for quick validation without querying GCP for list of regions or machine types.</li> <li><code>WANNA_GCP_CLOUD_BUILD_ACCESS_ALLOWED</code> allows using cloud build instead of local docker build.</li> <li>Default true.</li> <li>Disable for local docker build if you can't or don't want to use the cloud build.</li> <li><code>WANNA_OVERWRITE_DOCKER_IMAGE</code> overwrites the docker image name in the repository.</li> <li>Default true.</li> <li>Disable for docker image repositories which don't allow overwriting the image.</li> <li><code>WANNA_ALWAYS_OVERWRITE_DOCKER_TAGS</code> sets which tags should be overwritten every time. Useful for e.g. <code>latest</code> tag, which usually can be overwritten even when other tags can not. Set them as comma-separated list. This variable is used only when <code>WANNA_OVERWRITE_DOCKER_IMAGE</code> is set to false.</li> <li>Default <code>latest</code>.</li> </ul>"},{"location":"configuration/#docker-push-configuration","title":"Docker push configuration","text":"<p>This section covers configuration of docker image name, including repository.</p> <p>Following env vars are available. They all are optional and don't have default:</p> <ul> <li><code>WANNA_DOCKER_REGISTRY_SUFFIX</code> optional path in the registry</li> <li><code>WANNA_DOCKER_REGISTRY</code> overrides <code>docker.registry</code> and <code>gcp_profile.docker_registry</code> in <code>wanna.yaml</code></li> <li>if none are specified, the registry <code>{gcp_profiles.region}-docker.pkg.dev</code> is used.</li> <li><code>WANNA_DOCKER_REGISTRY_REPOSITORY</code> overrides <code>docker.repository</code> and <code>gcp_profile.docker_repository</code> in <code>wanna.yaml</code></li> <li><code>WANNA_DOCKER_REGISTRY_PROJECT_ID</code> overrides <code>gcp_profiles.project_id</code> in <code>wanna.yaml</code></li> </ul> <p>The whole algorithm is a bit complex and can be seen in docker.py, but if all variables are specified, the final image name is:</p> <pre><code>WANNA_DOCKER_REGISTRY / WANNA_DOCKER_REGISTRY_SUFFIX / WANNA_DOCKER_REGISTRY_PROJECT_ID / WANNA_DOCKER_REGISTRY_REPOSITORY / wanna_project.name / docker.images.name\n</code></pre>"},{"location":"configuration/#others","title":"Others","text":"<ul> <li><code>WANNA_IMPERSONATE_ACCOUNT</code> sets SA for impersonation</li> <li>Required in some CI environments if the automated mechanism for impersonation does not work.</li> <li><code>WANNA_GCP_PROFILE_PATH</code> can be used to load GCP profiles from outside of <code>wanna.yaml</code> file.</li> </ul>"},{"location":"development/","title":"Development","text":""},{"location":"development/#development-environment","title":"Development environment","text":"<p>This package uses poetry, instantiate everything by</p> <pre><code>poetry install --all-extras\n</code></pre> <p>in order to update dependencies, do changes in <code>pyproject.toml</code> and run</p> <pre><code>poetry lock --no-update\npoetry install --all-extras\n</code></pre>"},{"location":"development/#formatting","title":"Formatting","text":"<p>You can run the formatting by</p> <pre><code>poetry run poe format-code\n</code></pre> <p>which uses Poe task to format the whole project using ruff.</p>"},{"location":"development/#static-analysis","title":"Static analysis","text":"<p>This package uses mypy, you can run it by  </p> <pre><code>poetry run mypy . --config-file pyproject.toml\n</code></pre>"},{"location":"development/#running-all-tests","title":"Running all tests","text":"<p>To run the tests, you can use the following command:</p> <pre><code>poetry run pytest\n</code></pre> <p>this runs static analysis, linter and tests.</p>"},{"location":"development/#generating-documentation","title":"Generating documentation","text":"<p>Make sure the documentation is correct by</p> <pre><code>poetry run mkdocs serve\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<p>To run wanna-ml, you need <code>docker</code> daemon, a <code>Python &gt;=3.9</code> environment setup and gcloud cli installed</p>"},{"location":"installation/#installing-with-pipx","title":"Installing with Pipx","text":"<p>The recommended way to install wanna-ml is to use pipx.</p> <p>pipx will install the package in isolation so you won\u2019t have conflicts with other packages in your environment.</p> <p>You can install wanna-ml like this:</p> <pre><code>pipx install wanna-ml \n</code></pre> <p>You can upgrade the package like this:</p> <pre><code>pipx upgrade wanna-ml\n</code></pre> <p>if you want to manage the project with poetry</p> <pre><code>pipx install poetry\n</code></pre>"},{"location":"installation/#installing-with-pip","title":"Installing with Pip","text":"<p>wanna-ml is a normal Python package and you can install it with pip:</p> <pre><code>pip install wanna-ml\n</code></pre> <p>Be aware, that installing it globally might cause conflicts with other installed package.</p> <p>You can solve this problem by using a pipenv environment:</p> <pre><code>pipenv local 3.8.12\n\npip install wanna-ml\n</code></pre> <p>You will need to add wanna-ml to your PATH.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#stack-traces","title":"Stack traces","text":"<p>Wanna-ml CLI interface uses typer package and  rich for showing help, stack traces etc.</p> <p>By default, the wanna-ml will show verbose stack trace containing all local variables which can simplify  the development, but can be too verbose sometimes,  see the docs for more details.</p> <p>The stack trace looks something like this:</p> <pre><code>\u2502 \u2502            timeout = None                                                                    \u2502 \u2502\n\u2502 \u2502 transcoded_request = {                                                                       \u2502 \u2502\n\u2502 \u2502                      \u2502   'uri': '/compute/v1/projects/your-gcp-project-id/regions',          \u2502 \u2502\n\u2502 \u2502                      \u2502   'query_params': ,                                                   \u2502 \u2502\n\u2502 \u2502                      \u2502   'method': 'get'                                                     \u2502 \u2502\n\u2502 \u2502                      }                                                                       \u2502 \u2502\n\u2502 \u2502                uri = '/compute/v1/projects/your-gcp-project-id/regions'                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nNotFound: 404 GET https://compute.googleapis.com/compute/v1/projects/your-gcp-project-id/regions: The resource 'projects/your-gcp-project-id' was not found\n</code></pre> <p>If you don't like this tabular stack trace, you can disable this behavior by setting environment variable</p> <pre><code>export _TYPER_STANDARD_TRACEBACK=1\n</code></pre> <p>in shell, or</p> <pre><code>$Env:_TYPER_STANDARD_TRACEBACK=1\n</code></pre> <p>in powershell. Then, the regular stack trace will be shown, like this:</p> <pre><code>  File \"C:\\Projects\\others\\wanna-ml\\src\\wanna\\core\\utils\\validators.py\", line 29, in validate_region\n    available_regions = get_available_regions(project_id=values.get(\"project_id\"))\n  File \"C:\\Projects\\others\\wanna-ml\\src\\wanna\\core\\utils\\gcp.py\", line 228, in get_available_regions\n    response = RegionsClient(credentials=get_credentials()).list(project=project_id)\n  File \"C:\\Users\\E10270\\.conda\\envs\\wanna-ml-py310\\lib\\site-packages\\google\\cloud\\compute_v1\\services\\regions\\client.py\", line 874, in list\n    response = rpc(\n  File \"C:\\Users\\E10270\\.conda\\envs\\wanna-ml-py310\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n  File \"C:\\Users\\E10270\\.conda\\envs\\wanna-ml-py310\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 76, in error_remapped_callable\n    return callable_(*args, **kwargs)\n  File \"C:\\Users\\E10270\\.conda\\envs\\wanna-ml-py310\\lib\\site-packages\\google\\cloud\\compute_v1\\services\\regions\\transports\\rest.py\", line 392, in __call__\n    raise core_exceptions.from_http_response(response)\ngoogle.api_core.exceptions.NotFound: 404 GET https://compute.googleapis.com/compute/v1/projects/your-gcp-project-id/regions: The resource 'projects/your-gcp-project-id' was not found\n</code></pre> <p>If you like the verbosity of the tabular stack trace, but it's too narrow, you can increase the width to an arbitrary number by setting the <code>COLUMNS</code> or <code>TERMINAL_WIDTH</code> environment variable, e.g.:</p> <pre><code>export COLUMNS=150\nexport TERMINAL_WIDTH=150\n</code></pre> <p>in shell, or </p> <pre><code>$Env:COLUMNS=150\n$Env:TERMINAL_WIDTH=150\n</code></pre> <p>Then, the stack trace will look like this:</p> <pre><code>\u2502 \u2502               self = _List(                                                                                                                    \u2502 \u2502\n\u2502 \u2502                      \u2502   _session=&lt;google.auth.transport.requests.AuthorizedSession object at 0x000001F4E9CA17B0&gt;,                             \u2502 \u2502\n\u2502 \u2502                      \u2502   _host='https://compute.googleapis.com',                                                                               \u2502 \u2502\n\u2502 \u2502                      \u2502   _interceptor=&lt;google.cloud.compute_v1.services.regions.transports.rest.RegionsRestInterceptor object at               \u2502 \u2502\n\u2502 \u2502                      0x000001F4E9CA15A0&gt;                                                                                                       \u2502 \u2502\n\u2502 \u2502                      )                                                                                                                         \u2502 \u2502\n\u2502 \u2502            timeout = None                                                                                                                      \u2502 \u2502\n\u2502 \u2502 transcoded_request = {'uri': '/compute/v1/projects/your-gcp-project-id/regions', 'query_params': , 'method': 'get'}                            \u2502 \u2502\n\u2502 \u2502                uri = '/compute/v1/projects/your-gcp-project-id/regions'                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nNotFound: 404 GET https://compute.googleapis.com/compute/v1/projects/your-gcp-project-id/regions: The resource 'projects/your-gcp-project-id' was not\nfound\n</code></pre>"},{"location":"cli/commands/","title":"CLI Reference","text":"<p>This page provides documentation for wanna command-line tool.</p>"},{"location":"cli/commands/#wanna","title":"wanna","text":"<p>Complete MLOps framework for Vertex-AI</p> <p>Usage:</p> <pre><code> [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --install-completion  Install completion for the current shell.\n  --show-completion     Show completion for the current shell, to copy it or\n                        customize the installation.\n</code></pre>"},{"location":"cli/commands/#init","title":"init","text":"<p>Initiate a new wanna-ml project from template.</p> <p>Usage:</p> <pre><code> init [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --output-dir PATH      The output directory where wanna-ml repository will\n                         be created  \\[required]\n  -t, --template TEXT    The git repository of the template you want to use\n                         \\[required]\n  -c, --checkout TEXT    The branch, tag or commit to checkout after cloning\n                         the repository\n  -d, --directory TEXT   The directory within the repository to use as the\n                         template\n  --overwrite-if-exists  Overwrite the contents of the output directory if it\n                         exists\n  --no-input             Do not prompt for parameters and only use\n                         cookiecutter.json file content\n</code></pre>"},{"location":"cli/commands/#job","title":"job","text":"<p>Plugin for building and deploying training jobs.</p> <p>Usage:</p> <pre><code> job [OPTIONS] COMMAND [ARGS]...\n</code></pre>"},{"location":"cli/commands/#build","title":"build","text":"<p>Create a manifest based on the wanna-ml config that can be later pushed or run.</p> <p>Usage:</p> <pre><code> job build [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  -n, --name TEXT     Specify only one job from your wanna-ml yaml\n                      configuration to build. Choose 'all' to build all jobs.\n                      \\[default: all]\n</code></pre>"},{"location":"cli/commands/#push","title":"push","text":"<p>Build and push manifest to Cloud Storage.</p> <p>Usage:</p> <pre><code> job push [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH                 Path to the wanna-ml yaml configuration. Use\n                                  -f - to pass the configuration through\n                                  stdin.  \\[env var: WANNA_FILE; default:\n                                  wanna.yaml]\n  -p, --profile TEXT              Name of the GCP profile you want to use.\n                                  Profiles are loaded from wanna-ml yaml\n                                  config and (optionally) from this file too.\n                                  \\[env var: WANNA_GCP_PROFILE_NAME; default:\n                                  default]\n  -v, --version TEXT              job version  \\[default: dev]\n  -n, --name TEXT                 Specify only one job from your wanna-ml yaml\n                                  configuration to push. Choose 'all' to push\n                                  all jobs.  \\[default: all]\n  -m, --mode [all|manifests|containers|quick]\n                                  Push mode, this is useful if you want to\n                                  push containers in one step and deploy\n                                  instances in other.Use all for dev\n                                  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#report","title":"report","text":"<p>Displays a link to the cost report per wanna_project and optionally per job name.</p> <p>Usage:</p> <pre><code> job report [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  -n, --name TEXT     Specify only one job from your wanna-ml yaml\n                      configuration to report. Choose 'all' to report all\n                      jobs.  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#run","title":"run","text":"<p>Run the job as specified in wanna-ml config. This command puts together build, push and run-manifest steps.</p> <p>Usage:</p> <pre><code> job run [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH        Path to the wanna-ml yaml configuration. Use -f - to\n                         pass the configuration through stdin.  \\[env var:\n                         WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT     Name of the GCP profile you want to use. Profiles are\n                         loaded from wanna-ml yaml config and (optionally)\n                         from this file too.  \\[env var:\n                         WANNA_GCP_PROFILE_NAME; default: default]\n  -v, --version TEXT     job version  \\[default: dev]\n  -n, --name TEXT        Specify only one job from your wanna-ml yaml\n                         configuration to run. Choose 'all' to run all jobs.\n                         \\[default: all]\n  -hp, --hp-params PATH  Path to the params file in yaml format\n  -s, --sync             Runs the job in sync mode\n</code></pre>"},{"location":"cli/commands/#run-manifest","title":"run-manifest","text":"<p>Run the job as specified in the wanna-ml manifest.</p> <p>Usage:</p> <pre><code> job run-manifest [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -v, --manifest TEXT    Job deployment manifest\n  -hp, --hp-params PATH  Path to the params file in yaml format\n  -s, --sync             Runs the pipeline in sync mode\n</code></pre>"},{"location":"cli/commands/#stop","title":"stop","text":"<p>Stop a running job.</p> <p>Usage:</p> <pre><code> job stop [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  -n, --name TEXT     Specify only one job from your wanna-ml yaml\n                      configuration to stop. Choose 'all' to stop all jobs.\n                      \\[default: all]\n</code></pre>"},{"location":"cli/commands/#notebook","title":"notebook","text":"<p>Create, delete and more operations for Workbench Instance (Jupyter notebook).</p> <p>Usage:</p> <pre><code> notebook [OPTIONS] COMMAND [ARGS]...\n</code></pre>"},{"location":"cli/commands/#build_1","title":"build","text":"<p>Validates build of notebooks as they are defined in wanna.yaml</p> <p>Usage:</p> <pre><code> notebook build [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  -v, --version TEXT  notebook version  \\[default: dev]\n</code></pre>"},{"location":"cli/commands/#create","title":"create","text":"<p>Workbench Instance.</p> <p>If there already is a notebook with the same name in the same location and project, you will be prompt if you want to delete the existing one and start a new one.</p> <p>When the notebook instance is created, you will be given a URL link to JupyterLab.</p> <p>Usage:</p> <pre><code> notebook create [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH                 Path to the wanna-ml yaml configuration. Use\n                                  -f - to pass the configuration through\n                                  stdin.  \\[env var: WANNA_FILE; default:\n                                  wanna.yaml]\n  -p, --profile TEXT              Name of the GCP profile you want to use.\n                                  Profiles are loaded from wanna-ml yaml\n                                  config and (optionally) from this file too.\n                                  \\[env var: WANNA_GCP_PROFILE_NAME; default:\n                                  default]\n  -n, --name TEXT                 Specify only one workbench instance from\n                                  your wanna-ml yaml configuration to create.\n                                  Choose 'all' to create all workbench\n                                  instances.  \\[default: all]\n  -o, --owner TEXT\n  -v, --version TEXT              workbench instance version  \\[default: dev]\n  -m, --mode [all|manifests|containers|quick]\n                                  Push mode, this is useful if you want to\n                                  push containers in one step and deploy\n                                  instances in other.Use all for dev\n                                  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#delete","title":"delete","text":"<p>Workbench Instance.</p> <p>Usage:</p> <pre><code> notebook delete [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  -n, --name TEXT     Specify only one workbench instance from your wanna-ml\n                      yaml configuration to delete. Choose 'all' to delete all\n                      workbench instances.  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#push_1","title":"push","text":"<p>Push docker containers. This command also builds the images.</p> <p>Usage:</p> <pre><code> notebook push [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH                 Path to the wanna-ml yaml configuration. Use\n                                  -f - to pass the configuration through\n                                  stdin.  \\[env var: WANNA_FILE; default:\n                                  wanna.yaml]\n  -p, --profile TEXT              Name of the GCP profile you want to use.\n                                  Profiles are loaded from wanna-ml yaml\n                                  config and (optionally) from this file too.\n                                  \\[env var: WANNA_GCP_PROFILE_NAME; default:\n                                  default]\n  -n, --name TEXT                 Specify only one workbench instance from\n                                  your wanna-ml yaml configuration to push.\n                                  Choose 'all' to push all workbench\n                                  instances.  \\[default: all]\n  -v, --version TEXT              notebook version  \\[default: dev]\n  -m, --mode [all|manifests|containers|quick]\n                                  Notebook push mode, due to CI/CD not\n                                  allowing to push to docker registry from GCP\n                                  Agent, we need to split it. Notebooks\n                                  currently support only containers, as we do\n                                  not create manifests as of now.  \\[default:\n                                  containers]\n</code></pre>"},{"location":"cli/commands/#report_1","title":"report","text":"<p>Displays a link to the cost report per wanna_project and optionally per instance name.</p> <p>Usage:</p> <pre><code> notebook report [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  -n, --name TEXT     Specify only one notebook from your wanna-ml yaml\n                      configuration to report. Choose 'all' to report all\n                      notebooks.  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#ssh","title":"ssh","text":"<p>SSH connect to the Compute Engine instance that is behind the Jupyter Notebook.</p> <p>This will only work if the notebook is already running.</p> <p>Please note that you can connect to only one instance with one command call. If you have more notebooks defined in your YAML config, you have to select to which you want to connect to, instance_name \"all\" will be refused.</p> <p>Usage:</p> <pre><code> notebook ssh [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH                 Path to the wanna-ml yaml configuration. Use\n                                  -f - to pass the configuration through\n                                  stdin.  \\[env var: WANNA_FILE; default:\n                                  wanna.yaml]\n  -p, --profile TEXT              Name of the GCP profile you want to use.\n                                  Profiles are loaded from wanna-ml yaml\n                                  config and (optionally) from this file too.\n                                  \\[env var: WANNA_GCP_PROFILE_NAME; default:\n                                  default]\n  -n, --name TEXT                 Specify to which notebook you want to\n                                  connect via ssh. Selecting 'all' will work\n                                  only if there is just one notebook defined\n                                  in your configuration, an error will be\n                                  thrown otherwise.  \\[default: all]\n  -b, --background / -i, --interactive\n                                  Interactive mode will start a bash directly\n                                  in the Compute Engine instance backing the\n                                  Jupyter notebook. Background mode serves\n                                  more like a port-forwarding, you will be\n                                  able to connect to the Jupyter Lab at\n                                  localhost:{LOCAL_PORT}  \\[default:\n                                  interactive]\n  --port INTEGER                  Jupyter Lab will be accessible at this port\n                                  at localhost.  \\[default: 8080]\n</code></pre>"},{"location":"cli/commands/#sync","title":"sync","text":"<p>Synchronize existing User-managed Notebooks with wanna.yaml</p> <ol> <li>Reads current notebooks where label is defined per field wanna_project.name in wanna.yaml</li> <li>Does a diff between what is on GCP and what is on yaml</li> <li>Create the ones defined in yaml and missing in GCP</li> <li>Delete the ones in GCP that are not in wanna.yaml</li> </ol> <p>Usage:</p> <pre><code> notebook sync [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH                 Path to the wanna-ml yaml configuration. Use\n                                  -f - to pass the configuration through\n                                  stdin.  \\[env var: WANNA_FILE; default:\n                                  wanna.yaml]\n  -p, --profile TEXT              Name of the GCP profile you want to use.\n                                  Profiles are loaded from wanna-ml yaml\n                                  config and (optionally) from this file too.\n                                  \\[env var: WANNA_GCP_PROFILE_NAME; default:\n                                  default]\n  --force                         Synchronisation without prompt\n  -v, --version TEXT              notebook version  \\[default: dev]\n  -m, --mode [all|manifests|containers|quick]\n                                  Push mode, this is useful if you want to\n                                  push containers in one step and deploy\n                                  instances in other.Use all for dev\n                                  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#pipeline","title":"pipeline","text":"<p>Plugin for building and deploying Vertex-AI ML Pipelines.</p> <p>Usage:</p> <pre><code> pipeline [OPTIONS] COMMAND [ARGS]...\n</code></pre>"},{"location":"cli/commands/#build_2","title":"build","text":"<p>Create a manifest based on the wanna-ml config that can be later pushed, deployed or run.</p> <p>Usage:</p> <pre><code> pipeline build [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -v, --version TEXT              pipeline version  \\[default: dev]\n  --params PATH                   Path to the params file in yaml format\n                                  \\[env var: WANNA_ENV_PIPELINE_PARAMS]\n  -f, --file PATH                 Path to the wanna-ml yaml configuration. Use\n                                  -f - to pass the configuration through\n                                  stdin.  \\[env var: WANNA_FILE; default:\n                                  wanna.yaml]\n  -p, --profile TEXT              Name of the GCP profile you want to use.\n                                  Profiles are loaded from wanna-ml yaml\n                                  config and (optionally) from this file too.\n                                  \\[env var: WANNA_GCP_PROFILE_NAME; default:\n                                  default]\n  -n, --name TEXT                 Specify only one pipeline from your wanna-ml\n                                  yaml configuration to compile. Choose 'all'\n                                  to compile all pipelines.  \\[default: all]\n  -m, --mode [all|manifests|containers|quick]\n                                  Push mode, this is useful if you want to\n                                  push containers in one step and deploy\n                                  instances in other.Use all for dev\n                                  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#deploy","title":"deploy","text":"<p>Deploy the pipeline. Deploying means you can set a schedule and the pipeline will not be run only once, but on regular basis.</p> <p>Usage:</p> <pre><code> pipeline deploy [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -v, --version TEXT  pipeline version  \\[default: dev]\n  -e, --env TEXT      Pipeline env  \\[env var: WANNA_ENV; default: local]\n  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  -n, --name TEXT     Specify only one pipeline from your wanna-ml yaml\n                      configuration to deploy. Choose 'all' to deploy all\n                      pipelines.  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#push_2","title":"push","text":"<p>Build and push manifest to Cloud Storage.</p> <p>Usage:</p> <pre><code> pipeline push [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -v, --version TEXT              pipeline version  \\[default: dev]\n  --params PATH                   Path to the params file in yaml format\n                                  \\[env var: WANNA_ENV_PIPELINE_PARAMS]\n  -f, --file PATH                 Path to the wanna-ml yaml configuration. Use\n                                  -f - to pass the configuration through\n                                  stdin.  \\[env var: WANNA_FILE; default:\n                                  wanna.yaml]\n  -p, --profile TEXT              Name of the GCP profile you want to use.\n                                  Profiles are loaded from wanna-ml yaml\n                                  config and (optionally) from this file too.\n                                  \\[env var: WANNA_GCP_PROFILE_NAME; default:\n                                  default]\n  -n, --name TEXT                 Specify only one pipeline from your wanna-ml\n                                  yaml configuration to push. Choose 'all' to\n                                  push all pipelines.  \\[default: all]\n  -m, --mode [all|manifests|containers|quick]\n                                  Push mode, this is useful if you want to\n                                  push containers in one step and deploy\n                                  instances in other.Use all for dev\n                                  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#report_2","title":"report","text":"<p>Displays a link to the cost report per wanna_project and optionally per instance name.</p> <p>Usage:</p> <pre><code> pipeline report [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  -n, --name TEXT     Specify only one pipeline from your wanna-ml yaml\n                      configuration to report. Choose 'all' to report all\n                      pipelines.  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#run_1","title":"run","text":"<p>Run the pipeline as specified in wanna-ml config. This command puts together build, push and run-manifest steps.</p> <p>Usage:</p> <pre><code> pipeline run [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -v, --version TEXT              pipeline notebook version  \\[default: dev]\n  --params PATH                   Path to the params file in yaml format\n                                  \\[env var: WANNA_ENV_PIPELINE_PARAMS]\n  -s, --sync                      Runs the pipeline in sync mode\n  -f, --file PATH                 Path to the wanna-ml yaml configuration. Use\n                                  -f - to pass the configuration through\n                                  stdin.  \\[env var: WANNA_FILE; default:\n                                  wanna.yaml]\n  -p, --profile TEXT              Name of the GCP profile you want to use.\n                                  Profiles are loaded from wanna-ml yaml\n                                  config and (optionally) from this file too.\n                                  \\[env var: WANNA_GCP_PROFILE_NAME; default:\n                                  default]\n  -n, --name TEXT                 Specify only one pipeline from your wanna-ml\n                                  yaml configuration to run. Choose 'all' to\n                                  run all pipelines.  \\[default: all]\n  -m, --mode [all|manifests|containers|quick]\n                                  Push mode, this is useful if you want to\n                                  push containers in one step and deploy\n                                  instances in other.Use all for dev\n                                  \\[default: all]\n  --skip-execution-cache          configuration to skip kfp execution cache\n</code></pre>"},{"location":"cli/commands/#run-manifest_1","title":"run-manifest","text":"<p>Run the pipeline as specified in the wanna-ml manifest.</p> <p>Usage:</p> <pre><code> pipeline run-manifest [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -v, --manifest TEXT  Job deployment manifest\n  --params PATH        Path to the params file in yaml format  \\[env var:\n                       WANNA_ENV_PIPELINE_PARAMS]\n  -s, --sync           Runs the pipeline in sync mode\n</code></pre>"},{"location":"cli/commands/#tensorboard","title":"tensorboard","text":"<p>Create, delete or list Tensorboard instances.</p> <p>Usage:</p> <pre><code> tensorboard [OPTIONS] COMMAND [ARGS]...\n</code></pre>"},{"location":"cli/commands/#create_1","title":"create","text":"<p>Create Tensorboard Instance in GCP Vertex AI Experiments.</p> <p>If there already is a tensorboard with the same name in the same location and project, you will be prompt if you want to delete the existing one and start a new one.</p> <p>When the tensorboard instance is created, you will be given a full resource name.</p> <p>Usage:</p> <pre><code> tensorboard create [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  -n, --name TEXT     Specify only one tensorboard from your wanna-ml yaml\n                      configuration to create. Choose 'all' to create all\n                      tensorboards.  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#delete_1","title":"delete","text":"<p>Delete Tensorboard Instance in GCP Vertex AI Experiments.</p> <p>Usage:</p> <pre><code> tensorboard delete [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  -n, --name TEXT     Specify only one tensorboard from your wanna-ml yaml\n                      configuration to delete. Choose 'all' to delete all\n                      tensorboards.  \\[default: all]\n</code></pre>"},{"location":"cli/commands/#list","title":"list","text":"<p>list Tensorboard Instances in GCP Vertex AI Experiments.</p> <p>We also show Tensorboard Experiments and Tensorboard Runs for each Instance in the tree format.</p> <p>Usage:</p> <pre><code> tensorboard list [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -f, --file PATH     Path to the wanna-ml yaml configuration. Use -f - to\n                      pass the configuration through stdin.  \\[env var:\n                      WANNA_FILE; default: wanna.yaml]\n  -p, --profile TEXT  Name of the GCP profile you want to use. Profiles are\n                      loaded from wanna-ml yaml config and (optionally) from\n                      this file too.  \\[env var: WANNA_GCP_PROFILE_NAME;\n                      default: default]\n  --region TEXT       Overwrites the region from wanna-ml yaml configuration\n  --filter TEXT       GCP filter expression for tensorboard instances. Read\n                      more on GCP filters on https://cloud.google.com/sdk/gclo\n                      ud/reference/topic/filters  Example: display_name=my-\n                      tensorboard.  Example: labels.wanna_project:* - to show\n                      all tensorboard created by wanna-ml. Example:\n                      labels.wanna_project:sushi-ssl.\n  --url / --no-url    Weather to show URL link to experiments  \\[default: url]\n</code></pre>"},{"location":"cli/commands/#version","title":"version","text":"<p>Print your current and latest available version</p> <p>Usage:</p> <pre><code> version [OPTIONS]\n</code></pre>"},{"location":"tutorial/","title":"WANNA - Get started","text":""},{"location":"tutorial/#installation","title":"Installation","text":"<p>Install using <code>pip install -U wanna-ml</code>.</p> <p>For more information on the installation process and requirements, visit out installation page in documentation</p>"},{"location":"tutorial/#authentication","title":"Authentication","text":"<p>WANNA-ML relies on <code>gcloud</code> for user authentication. </p> <ol> <li>Install the <code>gcloud</code> CLI - follow official guide</li> <li>Authenticate with the <code>gcloud init</code></li> <li>Set you Google Application Credentials <code>gcloud auth application-default login</code></li> </ol>"},{"location":"tutorial/#docker-build","title":"Docker Build","text":"<p>You can use a local Docker daemon to build Docker images, but it is not required.  You are free to choose between local building on GCP Cloud Build.  If you prefer local Docker image building, install  Docker Desktop.</p>"},{"location":"tutorial/#gcp-iam-roles-and-permissions","title":"GCP IAM Roles and Permissions","text":"<p>Different WANNA-ML calls require different GCP permissions to create given resources on GCP. Our documentation page lists recommended GCP IAM roles for each <code>wanna</code> command.</p>"},{"location":"tutorial/#passing-configuration","title":"Passing configuration","text":"<p>Most of the commands are accepting <code>--file &lt;path_to_yaml&gt;</code> parameter, or <code>-f &lt;path_to_yaml&gt;</code> for short. It can be passed using environment variable <code>WANNA_FILE</code> as well. If you have the yaml configuration dynamic, and use .e.g. <code>cat wanna.yaml | envsubstr</code> to replace environment variables,  you can use <code>-</code> as a file name, so the usage would be <code>cat wanna.yaml | envsubstr | wanna &lt;command&gt; -f -</code>.</p>"},{"location":"tutorial/docker/","title":"WANNA Docker","text":"<p>Multiple resources created by WANNA rely on Docker containers. We make it easy for you to build your images either locally or using GCP Cloud Build.</p>"},{"location":"tutorial/docker/#types-of-docker-images","title":"Types of docker images","text":"<p>We currently support three types of docker images:</p> <ul> <li><code>provided_image</code> - you supply a link to the docker image in the registry. We don't build anything,   just redirect this link to GCP.</li> <li><code>local_build_image</code> - you supply a Dockerfile with a context directory and additional information.   We build the image for you on your machine or in the cloud.</li> <li><code>notebook_ready_image</code> - you supply a list of pip requirements to install in your Jupyter Notebook. This is useful if you want to start a notebook with custom libraries, but you don't want to handle   Dockerfile information.</li> </ul>"},{"location":"tutorial/docker/#referencing-docker-images","title":"Referencing docker images","text":"<p>Each docker image must have a <code>name</code>. By this name, you can later reference it in  resource configuration, usually as <code>docker_image_ref</code>.</p> <p>Example:</p> <pre><code>docker:\n  images:\n    - build_type: local_build_image\n      name: custom-notebook-container-julia\n      context_dir: .\n      dockerfile: Dockerfile.notebook\n  repository: wanna-samples\n  cloud_build: true\n\nnotebooks:\n  - name: wanna-notebook-julia\n    environment:\n      docker_image_ref: custom-notebook-container-julia\n</code></pre>"},{"location":"tutorial/docker/#local-build-vs-gcp-cloud-build","title":"Local build vs GCP Cloud Build","text":"<p>By default, all docker images are built locally on your machine and then pushed to the registry. For faster testing lifecycle you can build images directly using GCP Cloud Build.  The only needed change is to set <code>cloud_build: true</code> in <code>docker</code> section of the WANNA yaml config or set <code>WANNA_DOCKER_BUILD_IN_CLOUD=true</code> (env variable takes precedence).</p> <p>Building in the cloud is generally faster as the docker images are automatically already in the registry and there is no need to push the images over the network. That makes it suitable for fast testing.  However, building images in the cloud is not allowed for production.</p>"},{"location":"tutorial/docker/#build-configuration","title":"Build configuration","text":"<p>When building locally, we offer you a way to set additional build parameters. These parameters must be specified in a separate yaml file in path <code>WANNA_DOCKER_BUILD_CONFIG</code>. If this is not set, it defaults to the <code>dockerbuild.yaml</code> in the working directory.</p> <p>These parameters refer to standard docker build parameters.</p> <p>One example use case can be when you want to git clone your internal repository during the docker build.</p> <p>In the <code>dockerbuild.yaml</code>:</p> <pre><code>ssh: github=~/.ssh/id_rsa\n</code></pre> <p>In the <code>Dockerfile</code>:</p> <pre><code>RUN mkdir -m 700 /root/.ssh; \\\n  touch -m 600 /root/.ssh/known_hosts; \\\n  ssh-keyscan git.int.avast.com &gt; /root/.ssh/known_hosts\n\nRUN --mount=type=ssh,id=github git clone git@git.your.company.com:your_profile/your_repo.git\n</code></pre>"},{"location":"tutorial/docker/#parameters-for-docker-section","title":"Parameters for docker section","text":"class <code>wanna.core.models.docker.DockerModel</code>(*, images=, repository=None, registry=None, cloud_build_timeout=12000, cloud_build=False, cloud_build_workerpool=None, cloud_build_workerpool_location=None, cloud_build_kaniko_version='latest', cloud_build_kaniko_flags=) <ul> <li><code>images</code>- [list[Union[LocalBuildImageModel, ProvidedImageModel, NotebookReadyImageModel]]] Docker images that will be used in wanna-ml resources</li> <li><code>repository</code> - [str] (optional) GCP Artifact Registry repository for pushing images</li> <li><code>registry</code> - [str] (optional) GCP Artifact Registry, when not set it defaults to <code>{gcp_profile.region}-docker.pkg.dev</code></li> <li><code>cloud_build_timeout</code> - [int] <code>12000</code> how many seconds before cloud build timeout</li> <li><code>cloud_build</code> - [str] (optional) <code>false</code> (default) to build locally, <code>true</code> to use GCP Cloud Build</li> <li><code>cloud_build_workerpool</code> - [str] (optional) Name of the GCP Cloud Build workerpool if you want to use one</li> <li><code>cloud_build_workerpool_location</code> - [str] (optional) Location of the GCP Cloud Build workerpool. Must be specified if cloud_build_workerpool is set.</li> <li><code>cloud_build_kaniko_version</code> - [str] (optional) which https://github.com/GoogleContainerTools/kaniko/ version to use</li> <li><code>cloud_build_kaniko_flags</code> - [str] (optional) which https://github.com/GoogleContainerTools/kaniko/ flags to use</li> </ul> class <code>wanna.core.models.docker.ProvidedImageModel</code>(*, name, build_type, image_url) <ul> <li><code>build_type</code> - [str] \"provided_image\"</li> <li><code>name</code> - [str] This will later be used in <code>docker_image_ref</code> in other resources</li> <li><code>image_url</code> - [str] URL link to the image</li> </ul> class <code>wanna.core.models.docker.LocalBuildImageModel</code>(*, name, build_type, build_args=None, context_dir, dockerfile) <ul> <li><code>build_type</code> - [str] \"local_build_image\"</li> <li><code>name</code> - [str] This will later be used in <code>docker_image_ref</code> in other resources</li> <li><code>build_args</code> [dict[str, str]] - (optional) docker build args</li> <li><code>context_dir</code> [Path] - Path to the docker build context directory</li> <li><code>dockerfile</code> [Path] - Path to the Dockerfile</li> </ul> class <code>wanna.core.models.docker.NotebookReadyImageModel</code>(*, name, build_type, build_args=None, base_image='gcr.io/deeplearning-platform-release/base-cpu', requirements_txt) <ul> <li><code>build_type</code> - [str] \"notebook_ready_image\"</li> <li><code>name</code> - [str] This will later be used in <code>docker_image_ref</code> in other resources</li> <li><code>build_args</code> [dict[str, str]] - (optional) docker build args</li> <li><code>base_image</code> [str] - (optional) base notebook docker image, you can check available images https://cloud.google.com/deep-learning-vm/docs/images   when not set, it defaults to standard base CPU notebook.</li> <li><code>requirements_txt</code> [Path] - Path to the <code>requirements.txt</code> file containing python packages that will be installed</li> </ul>"},{"location":"tutorial/docker/#roles-and-permissions","title":"Roles and permissions","text":"<p>Permission and suggested roles (applying the principle of least privilege) required for docker images manipulation:</p> WANNA action Permissions Suggested Roles build in Cloud Build <code>cloudbuild.builds.create</code> and more <code>roles/cloudbuild.builds.builder</code> push <code>artifactregistry.repositories.uploadArtifacts</code>, <code>artifactregistry.tags.create</code>, <code>artifactregistry.tags.update</code> <code>roles/artifactregistry.writer</code> <p>For building the docker images locally, you will need permission to push to GCP as described above and running local Docker daemon. You also have to authenticate docker with GCP, detailed documentation is here. But generally, you should be fine with running:</p> <pre><code>gcloud auth login\n\ngcloud auth configure-docker europe-west1-docker.pkg.dev # Add more comma-separated repository hostnames if you wish\n</code></pre> <p>Full list of available roles and permission.</p>"},{"location":"tutorial/job/","title":"WANNA Job","text":"class <code>wanna.core.models.training_custom_job.BaseCustomJobModel</code>(*, name, project_id, zone=None, region, labels=None, description=None, service_account=None, network=None, bucket, tags=None, metadata=None, enable_web_access=False, base_output_directory=None, tensorboard_ref=None, timeout_seconds=86400, encryption_spec=None, env_vars=None) <ul> <li><code>name</code> - [str] Custom name for this instance</li> <li><code>project_id' - [str] (optional) Overrides GCP Project ID from the</code>gcp_profile` segment</li> <li><code>zone</code> - [str] (optional) Overrides zone from the <code>gcp_profile</code> segment</li> <li><code>region</code> - [str] (optional) Overrides region from the <code>gcp_profile</code> segment</li> <li><code>labels</code>- [dict[str, str]] (optional) Custom labels to apply to this instance</li> <li><code>service_account</code> - [str] (optional) Overrides service account from the <code>gcp_profile</code> segment</li> <li><code>network</code> - [str] (optional) Overrides network from the <code>gcp_profile</code> segment</li> <li><code>tags</code>- [dict[str, str]] (optional) Tags to apply to this instance</li> <li><code>metadata</code>- [str] (optional) Custom metadata to apply to this instance</li> <li><code>enable_web_access</code> - [bool] Whether you want Vertex AI to enable interactive shell access to training containers. Default is False</li> <li><code>bucket</code> - [str] Overrides bucket from the <code>gcp_profile</code> segment</li> <li><code>base_output_directory</code> - [str] (optional) Path to where outputs will be saved</li> <li><code>tensorboard_ref</code> - [str] (optional) Name of the Vertex AI Experiment</li> <li><code>timeout_seconds</code> - [int] Job timeout. Defaults to 60 * 60 * 24 s = 24 hours</li> <li><code>encryption_spec</code>- [str] (optional) The Cloud KMS resource identifier. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key The key needs to be in the same region as where the compute resource is created</li> <li><code>env_vars</code> - dict[str, str] (optional) Environment variables to be propagated to the job</li> </ul>"},{"location":"tutorial/job/#hyper-parameter-tuning","title":"Hyper-parameter tuning","text":"class <code>wanna.core.models.training_custom_job.HyperparameterTuning</code>(*, metrics, parameters, max_trial_count=15, parallel_trial_count=3, search_algorithm=None, encryption_spec=None) <ul> <li><code>metrics</code> - Dictionary of type [str, Literal[\"minimize\", \"maximize\"]]</li> <li><code>parameters</code> - list[HyperParamater] defined per var_name, type, min, max, scale</li> <li><code>max_trial_count</code> - [int] defaults to 15</li> <li><code>parallel_trial_count</code> - [int] defaults to 3</li> <li><code>search_algorithm</code> - [str] (optional) Can be \"grid\" or \"random\"</li> <li><code>encryption_spec</code> - [str] (optional) The Cloud KMS resource identifier. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key The key needs to be in the same region as where the compute resource is created</li> </ul> <p>A custom job can be simply converted to a hyper-parameter tuning job just by adding  one extra parameter called <code>hp_tuning</code>. This will start a series of jobs (instead of just one job)  and try to find the best combination of hyper-parameters in regard to a target variable that you specify.</p> <p>Read the official documentation for more information.</p> <p>In general, you have to set which hyper-parameters are changeable, which metric you want to optimize over and how many trials you want to run. You also need to adjust your training script so it would accept hyper-parameters as script arguments and report the optimized metric back to Vertex-Ai.</p>"},{"location":"tutorial/job/#setting-hyper-parameter-space","title":"Setting hyper-parameter space","text":"<p>Your code should accept a script arguments with name matching <code>wanna.yaml</code> config. For example, if you want to fine-tune the learning rate in your model:</p> <p>In <code>wanna.yaml</code> config:</p> <pre><code>    hp_tuning:\n      parameters:\n        - var_name: learning_rate\n          type: double\n          min: 0.001\n          max: 1\n          scale: log\n</code></pre> <p>And the python script should accept the same argument with the same type:</p> <pre><code>  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '--learning_rate',\n      required=True,\n      type=float,\n      help='learning rate')\n</code></pre> <p>Currently, you can use parameters of type <code>double</code>, <code>integer</code>, <code>discrete</code> and <code>categorical</code>. Each of them must be specified by <code>var_name</code>, <code>type</code> and additionaly:</p> <ul> <li><code>double</code>: <code>min</code>, <code>max</code> and <code>scale</code> (<code>linear</code> / <code>log</code>)</li> <li><code>integer</code>: <code>min</code>, <code>max</code> and <code>scale</code> (<code>linear</code> / <code>log</code>)</li> <li><code>discrete</code>: <code>values</code> (list of possible values)  and <code>scale</code> (<code>linear</code> / <code>log</code>)</li> <li><code>categorical</code>: <code>values</code> (list of possible values)</li> </ul>"},{"location":"tutorial/job/#setting-target-metric","title":"Setting target metric","text":"<p>You can choose to either maximize or minimize your optimized metric. Example in <code>wanna.yaml</code>:</p> <pre><code>    hp_tuning:\n      metrics: {'accuracy':'maximize'}\n      parameters:\n        ...\n</code></pre> <p>Your python script must report back the metric during training, you should use cloudml-hypertune library.</p> <pre><code>import hypertune\n\nhpt = hypertune.HyperTune()\nhpt.report_hyperparameter_tuning_metric(\n    hyperparameter_metric_tag='accuracy',\n    metric_value=0.987,\n    global_step=1000)\n</code></pre>"},{"location":"tutorial/job/#setting-number-of-trials-and-search-algorithm","title":"Setting number of trials and search algorithm","text":"<p>The number of trials can be influenced by <code>max_trial_count</code> and <code>parallel_trial_count</code>.</p> <p>Search through hyper-parameter space can be <code>grid</code>, <code>random</code> or if not any of those two are set, the default Bayesian Optimization will be used.</p>"},{"location":"tutorial/notebook/","title":"WANNA Notebook","text":"<p>We offer a simple way of managing Jupyter Notebooks on GCP using  Vertex AI Workbench Instances,  with multiple ways to set your environment, mount a GCS bucket, and more.</p> class <code>wanna.core.models.workbench.InstanceModel</code>(*, name, project_id, zone, region=None, labels=None, description=None, service_account=None, network=None, bucket=None, tags=None, metadata=None, machine_type='e2-standard-2', gpu=None, data_disk=None, subnet=None, tensorboard_ref=None, type='instance', owner=None, boot_disk=None, environment=NotebookEnvironment(vm_image=VMImage(version=None), docker_image_ref=None), no_public_ip=True, enable_dataproc=False, enable_ip_forwarding=False, no_proxy_access=False, enable_monitoring=True, idle_shutdown_timeout=720, collaborative=False, env_vars=None, bucket_mounts=None, post_startup_script=None, post_startup_script_behavior='run_once', environment_auto_upgrade=None, delete_to_trash=False, report_health=True) <ul> <li><code>type</code> - [str] (optional) Type of the notebook, instance, for dispatching purposes.</li> <li><code>name</code>- [str] Custom name for this instance</li> <li><code>project_id</code> - [str] (optional) Overrides GCP Project ID from the <code>gcp_profile</code> segment</li> <li><code>zone</code> - [str] (optional) Overrides zone from the <code>gcp_profile</code> segment</li> <li><code>region</code> - [str] (optional) Overrides region from the <code>gcp_profile</code> segment</li> <li><code>labels</code>- [dict[str, str]] (optional) Custom labels to apply to this instance</li> <li><code>service_account</code> - [str] (optional) Overrides service account from the <code>gcp_profile</code> segment</li> <li><code>network</code> - [str] (optional) Overrides network from the <code>gcp_profile</code> segment</li> <li><code>tags</code>- [dict[str, str]] (optional) Tags to apply to this instance</li> <li><code>metadata</code>- [Optional[dict[str, Any]]] (optional) Custom metadata to apply to this instance</li> <li><code>owner</code> - [str] This can be either a single user email address and that would be the only one   able to access the notebook. Or service account and then everyone who has the iam.serviceAccounts.actAs   permission on the specified service account will be able to connect.</li> <li><code>machine_type</code> - [str] (optional) GCP Compute Engine machine type</li> <li><code>gpu</code>- [GPU] (optional) The hardware GPU accelerator used on this instance.</li> <li><code>data_disk</code> - [Disk] (optional) Data disk configuration to attach to this instance.</li> <li><code>kernels</code> - [list[str]] (optional) Custom kernels given as links to container registry</li> <li><code>tensorboard_ref</code> - [str] (optional) Reference to Vertex Experimetes</li> <li><code>subnet</code>- [str] (optional) Subnetwork of a given network</li> <li><code>internal_ip_only</code> - [bool] (optional) Public or private (default) IP address</li> <li><code>idle_shutdown_timeout</code> - [int] (optional) Time in minutes, between 10 and 1440, defaults to 720. If None,     there is no idle shutdown</li> <li><code>post_startup_script</code> - [str] (optional) Path to a script that will be executed after the instance is started.</li> <li><code>post_startup_script_behavior</code> - [str] Defines the behavior of the post startup script.     Documentation https://cloud.google.com/vertex-ai/docs/workbench/instances/manage-metadata</li> <li><code>environment_auto_upgrade</code> - [str] (optional) Cron schedule for environment auto-upgrade.</li> <li><code>delete_to_trash</code> - [bool] (optional) If true, the instance will be deleted to trash.</li> <li><code>report_health</code> - [bool] (optional) If true, the instance will report health to Cloud Monitoring</li> </ul>"},{"location":"tutorial/notebook/#notebook-environments","title":"Notebook Environments","text":"<p>There are two distinct possibilities for your environment.</p> <ul> <li>Use a custom docker image, we recommend you build on top of GCP notebook ready images, either with using one of their images as a base or by using the <code>notebook_ready_image</code> docker type.    It is also possible to build your image from scratch, but please follow GCP's recommended    principles and port settings as described here.</li> </ul> <pre><code>docker:\n  images:\n    - build_type: local_build_image\n      name: custom-notebook-container\n      context_dir: .\n      dockerfile: Dockerfile.notebook\n  repository: wanna-samples\n  cloud_build: true\n\nnotebooks:\n  - name: wanna-notebook-custom-container\n    environment:\n      docker_image_ref: custom-notebook-container\n</code></pre> <ul> <li>Use a virtual machine image with preconfigured python libraries containing TensorFlow, PyTorch, R and more. Currently, GCP does not offer any customization, so you just pass empty dict to the <code>vm_image</code>.</li> </ul> <pre><code>notebooks:\n  - name: wanna-notebook-vm\n    machine_type: n1-standard-4\n    environment:\n     vm_image: {}\n</code></pre>"},{"location":"tutorial/notebook/#mounting-buckets","title":"Mounting buckets","text":"<p>We can automatically mount GCS buckets with <code>gcsfuse</code> during the notebook startup.</p> <p>Example:</p> <pre><code>    bucket_mounts:\n      - bucket_name: us-burger-gcp-poc-mooncloud\n        bucket_dir: data\n        local_path: /home/jupyter/mounted/gcs\n</code></pre>"},{"location":"tutorial/notebook/#tensorboard-integration","title":"Tensorboard integration","text":"<p><code>tb-gcp-uploader</code> is needed to upload the logs to the tensorboard instance. A detailed tutorial on this tool can be found here.</p> <p>If you set the <code>tensorboard_ref</code> in the WANNA yaml config, we will export the tensorboard resource name as <code>AIP_TENSORBOARD_LOG_DIR</code>.</p>"},{"location":"tutorial/notebook/#roles-and-permissions","title":"Roles and permissions","text":"<p>Permission and suggested roles (applying the principle of least privilege) required for notebook manipulation:</p> WANNA action Permissions Suggested Roles create See full list <code>roles/notebooks.runner</code>, <code>roles/notebooks.admin</code> delete see full list <code>roles/notebooks.admin</code> <p>For accessing the JupyterLab web interface, you must grant the user access to the service account used by the notebooks instance.  If the instance owner is set, only this user can access the web interface.</p> <p>Full list of available roles and permission.</p>"},{"location":"tutorial/notebook/#local-development-and-ssh","title":"Local development and SSH","text":"<p>If you wish to develop code in your local IDE and run it on Vertex-AI notebooks, we have a solution for you. Assuming your notebook is already running, you can set up an SSH connection via:</p> <pre><code>wanna notebook ssh --background -n notebook_name\n</code></pre> <p>Wanna will create an SSH tunnel using GCP IAP from your local environment to your notebook.</p> <p>The <code>--background/-b</code> flag means that the tunnel will be created in the background and you can  access the notebook running in GCP at <code>localhost:8080</code> (port can be customized with <code>--port</code>). The second possibility is to use <code>--interactive/-i</code> and that will start a bash inside the Compute Engine instance backing your Vertex-AI notebook.</p> <p>Once you set an <code>--background</code> connection to the notebook, you can use your favorite IDE to develop in the notebook. Here we share instructions on how to use VSCode for this.</p>"},{"location":"tutorial/notebook/#connecting-with-vscode","title":"Connecting with VSCode","text":"<ol> <li>Install Jupyter Extension </li> <li>Create a new file with the type Jupyter notebook</li> <li> <p>Select the Jupyter Server: local button in the global Status bar or run the     Jupyter: Specify local or remote Jupyter server for connections command from the Command Palette (\u21e7\u2318P).</p> </li> <li> <p>Select option <code>Existing URL</code> and input <code>http://localhost:8080</code></p> </li> <li>You should be connected. If you get an error saying something like <code>'_xsrf' argument missing from POST.</code>, it is because the VSCode cannot start a python kernel in GCP. The current workaround is to manually start    a kernel at <code>http://localhost:8080</code> and then in the VSCode connect to the exiting kernel in the right upper corner.</li> </ol> <p>A more detailed guide on setting a connection with VSCode to Jupyter can be found at https://code.visualstudio.com/docs/datascience/jupyter-notebooks.</p>"},{"location":"tutorial/notebook/#example","title":"Example","text":"<pre><code>notebooks:\n  - name: wanna-notebook-trial\n    service_account:\n    owner: \n    machine_type: n1-standard-4\n    labels:\n      notebook_usecase: wanna-notebook-sample-simple-pip\n    environment:\n      vm_image: {}\n    gpu:\n      count: 1\n      accelerator_type: NVIDIA_TESLA_V100\n      install_gpu_driver: true\n    boot_disk:\n      disk_type: pd_standard\n      size_gb: 100\n    data_disk:\n      disk_type: pd_standard\n      size_gb: 100\n    bucket_mounts:\n      - bucket_name: us-burger-gcp-poc-mooncloud\n        bucket_dir: data\n        local_path: /home/jupyter/mounted/gcs\n    tensorboard_ref: my-super-tensorboard\n</code></pre>"},{"location":"tutorial/pipeline/","title":"WANNA ML Pipelines","text":"<p>WANNA ML Pipelines aim at reducing the friction on development cycle to release whilst providing project organization for independent and testable components. It wrapps Kubeflow V2 pipelines with a build and deployment tools for managed GCP VertexAI ML Pipelines service. It has several utils and conventions to reduce boilerplate and speed up development.</p>"},{"location":"tutorial/pipeline/#tutorial-get-started","title":"Tutorial Get started","text":"<p>In this tutorial we will go over several steps that will bootstrap and run a Vertex AI Pipeline(Kubeflow V2) through wanna cli.</p>"},{"location":"tutorial/pipeline/#setup-environment","title":"Setup environment","text":"<pre><code># Login to GCP\ngcloud auth login\n\n# Auth against GCP docker registries\ngcloud auth configure-docker europe-west1-docker.pkg.dev \n\n# Create python env\nconda create -n pipeline-tutorial python=3.8 poetry\n\nconda activate pipeline-tutorial\n\npip install wanna-ml\n</code></pre>"},{"location":"tutorial/pipeline/#initialize-wanna-project","title":"Initialize WANNA project","text":"<pre><code>wanna init --template blank\n</code></pre> <p>For this turorial we will be using Avast cloud-lab-304213 GCP project as it is available to everyone.</p> <p>Answer the following questions:</p> <pre><code>project_name [project_name]: pipeline_tutorial\nproject_owner_fullname [project owner]:  \nproject_owner_email [you@avast.com]: \nproject_version [0.0.0]: \nproject_description [Link to WANNA project page on CML]: \nproject_slug [project_name]: \ngcp_project_id []: cloud-lab-304213\ngcp_service_account []: jacekhebdatest@cloud-lab-304213.iam.gserviceaccount.com\ngcp_bucket []: wanna-ml-west1-jh\n</code></pre> <p>Complete installation process</p> <pre><code>cd pipeline_tutorial\n\npoetry install\n</code></pre> <p>Build blank pipeline to check all is well in paradise</p> <pre><code>wanna pipeline build\n</code></pre>"},{"location":"tutorial/pipeline/#components","title":"Components","text":"<p>Now that we have a blank bootstraped Kubeflow V2 pipeline we need to add components. In this tutorial we choose to go with self contained components that are independent and testable. There is a recomended structure but it can be tedious to repeat every time and this is where <code>wanna</code> comes in to aliviate this boilerplate.</p>"},{"location":"tutorial/pipeline/#create-first-component-for-data-prep","title":"Create first component for data prep","text":"<p>The first component we will create is a data component where we can do some data preparation for the rest of the pipeline</p> <pre><code>wanna components create --output-dir pipeline/components/\n</code></pre> <p>the output should be something like</p> <pre><code>component_name [component_name]: data\ncomponent_author [component author]: Joao Da Silva\ncomponent_author_email [you@gendigital.com]: joao.silva1@gendigital.com\ncomponent_version [0.0.0]: \ncomponent_description [Describe your component]: data component for WANNA pipeline tutorial        \ncomponent_url [Link to MLOps project page]: \ncomponent_slug [data]: \ncomponent_docker_ref [data]: \nSelect component_framework:\n1 - base-cpu\n2 - base-cu110\n3 - base-cu113\n4 - pytorch-xla-1.11\n5 - pytorch-gpu-1.10\n6 - pytorch-gpu-1.11\n7 - sklearn-0.24\n8 - tf-cpu.2-6\n9 - tf-cpu.2-8\n10 - tf-gpu-slim.2-6\n11 - tf-gpu-slim.2-8\n12 - xgboost-cpu.1-1\nChoose from 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 [1]: 1\n</code></pre> <p>as a result you can see the following tree structure, which for the most part is a common python <code>lib</code> structure whith the exception of kubeflow's component.yaml</p> <pre><code>tree pipeline/components/data\n\npipeline/components/data\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 component.yaml\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 data.py\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 test_data.py\n\n4 directories, 9 files\n</code></pre> <p>You may also notice that a Dockerfile is present and here is where the python data <code>lib</code> is installed and executed. However we do need to add this Dockerfile to <code>wanna.yaml</code> so that wanna knows about it and can build it and export it.</p> <p>Update wanna <code>docker.images</code> yaml array with</p> <pre><code>images\n    - build_type: local_build_image\n      name: data\n      context_dir: pipeline/components/data/\n      dockerfile: pipeline/components/data/Dockerfile\n</code></pre> <p>and add the docker ref <code>data</code> into the pipeline in wanna.yaml so that wanna can link these and expose the docker tag and pipeline compine time. Update wanna.yamll path <code>pipelines[0].docker_image_ref</code> with <code>docker_image_ref: [data]</code>.</p> <p>It should now look like:</p> <pre><code>pipelines:\n  - name: pipeline-tutorial-pipeline\n    schedule:\n      cron: 2 * * * *\n    bucket: gs://wanna-tensorflow-sample-dev\n    pipeline_function: project_name.pipeline.wanna_pipeline\n    pipeline_params: pipeline/params.yaml\n    docker_image_ref: [\"data\"]\n    tensorboard_ref: pipeline-tutorial-board\n</code></pre> <p>Earlier I mentioned <code>expose the docker tag</code>, this means wanna exports every docker tag from <code>docker.images</code> array as <code>${NAME_DOCKER_URI}</code> so in this component you can see in the yaml <code>${DATA_DOCKER_URI}</code> which gets replaced at compile time, this way we have the possibility to have dynamic Kubeflow components versioned according to current pipeline release.</p> <p>Next we may wat to make this component part of our <code>poetry</code> build so that we can run tests, linter and whatnot from a single place.</p> <p>Lets edit our <code>pyproject.toml</code> as follows:</p> <pre><code>[tool.poetry.dependencies]\npython = \"&gt;=3.8,&lt;3.11\"\ndata = {path = \"pipeline/components/data\", develop=true}\n</code></pre> <p>from <code>poetry &gt;= 1.2</code> we  will be able to just run <code>poetry add pipeline/components/data --editable</code>.</p> <p>Let's install our data lib into the environment and run some</p> <pre><code>poetry lock &amp;&amp; poetry install\n</code></pre> <p>At this point running <code>python -m data.data --help</code> should show:</p> <pre><code>Usage: python -m data.data [OPTIONS]\n\nOptions:\n  --project TEXT\n  --location TEXT\n  --experiment-name TEXT\n  --help                  Show this message and exit.\n</code></pre> <p>as well as a quick test <code>pytest -s pipeline/components/data</code> or go full scale with pre-commit hook <code>git init . &amp;&amp; git add . &amp;&amp; task build</code>. This will fail on <code>flake8</code> as expected, as we will be using the var which flake8 complains about.</p> <p>You can now see that you have a self contained component and testable, ready to be added to the kubeflow pipeline.</p>"},{"location":"tutorial/pipeline/#pipeline","title":"Pipeline","text":"class <code>wanna.core.models.pipeline.PipelineModel</code>(*, name, project_id, zone, region=None, labels=None, description=None, service_account=None, network=None, bucket=None, tags=None, metadata=None, pipeline_function, pipeline_params=None, docker_image_ref=, schedule=None, tensorboard_ref=None, notification_channels_ref=, sla_hours=None, enable_caching=None, experiment=None) <ul> <li><code>name</code>- [str] Custom name for this instance</li> <li><code>project_id' - [str] (optional) Overrides GCP Project ID from the</code>gcp_profile` segment</li> <li><code>zone</code> - [str] (optional) Overrides zone from the <code>gcp_profile</code> segment</li> <li><code>region</code> - [str] (optional) Overrides region from the <code>gcp_profile</code> segment</li> <li><code>labels</code>- [dict[str, str]] (optional) Custom labels to apply to this instance</li> <li><code>service_account</code> - [str] (optional) Overrides service account from the <code>gcp_profile</code> segment</li> <li><code>network</code> - [str] (optional) Overrides network from the <code>gcp_profile</code> segment</li> <li><code>tags</code>- [dict[str, str]] (optional) Tags to apply to this instance</li> <li><code>metadata</code>- [str] (optional) Custom metadata to apply to this instance</li> <li><code>pipeline_function</code> - [str] (optional) Path to a cloud function</li> <li><code>pipeline_params</code> - [str] (optional) Path to params.yaml file</li> <li><code>docker_image_ref</code> - [list[str]] - list of names of docker images</li> <li><code>schedule</code> - [str] (optional) - Scheduler using a cron syntax</li> <li><code>tensorboard_ref</code> - [str] (optional) - Name of the Vertex AI Experiment</li> <li><code>notification_channels_ref</code> - [list[str]] (optional) list of names of notifications channel described by a model: (name: str, type: Literal[\"email\"], emails: list[EmailStr])</li> <li><code>sla_hours</code> - [float] (optional) Time after which the running pipeline gets stopped</li> <li><code>enable_caching</code> - [bool] enable KubeFlow pipeline execution caching</li> </ul> <p>WANNA template creates two files that allow to put together the Kubeflow V2 pipeline that will then be deployed to GCP Vertex AI Pipelines.</p> <p><code>pipeline/config.py</code> captures wanna compile time exposed environment variables and provides this as configuration to <code>pipeline/pipeline.py</code>.</p> <p>With that in mind let's add our data component to <code>pipeline.py</code>.</p> <p>First we imoprt wanna component loader that will replace ENV vars, namely the container</p> <pre><code>from wanna.components.loader import load_wanna_component\n</code></pre> <p>Secondly we will load the actual component. <code>wanna_pipeline</code> function should now look like this:</p> <pre><code>@dsl.pipeline(\n    # A name for the pipeline. Use to determine the pipeline Context.\n    name=cfg.PIPELINE_NAME,\n    pipeline_root=cfg.PIPELINE_ROOT\n)\ndef wanna_pipeline(eval_acc_threshold: float):\n    pipeline_dir = Path(__file__).parent.resolve()\n\n    # ===================================================================\n    # Get pipeline result notification\n    # ===================================================================\n    exit_task = (\n        on_exit()\n        .set_display_name(\"On Exit Dummy Task\")\n        .set_caching_options(False)\n    )\n\n    with dsl.ExitHandler(exit_task):\n        load_wanna_component(f\"{pipeline_dir}/components/data/component.yaml\")(\n            experiment_name=cfg.MODEL_DISPLAY_NAME\n        ).set_display_name(\"Data prep\")\n</code></pre> <p>now with everything in place, lets build the pipeline with <code>wanna pipeline build</code> or with <code>wanna pipeline build --quick</code> if you want to skip docker builds and just verify Kubeflow compiles and components have correct inputs and outputs connected.</p>"},{"location":"tutorial/pipeline/#running-the-pipeline-in-dev-mode","title":"Running the pipeline in <code>dev</code> mode","text":"<p>When WANNA runs a pipeline from local it will,</p> <ol> <li>build &amp; push the component containers to Google Docker registry</li> <li>compile the kubeflow pipeline and upload to gcs the resulting pipeline json spec</li> <li>compile and upload to gcs wanna manifest that allows to run the pipeline from anywhere</li> <li>Trigger the pipeline run and print its dashboard url and running state</li> </ol> <p>Assuming we are on <code>\u2714 Compiling pipeline pipeline-tutorial-pipeline</code> succeeded we can now actually run the pipeline.</p> <pre><code>wanna pipeline run --name pipeline-tutorial-pipeline --params pipeline/params.yaml --version dev --sync\n</code></pre> <p>if all goes with the plan you should see something along the lines(we will be improving the stdout logging overtime):</p> <pre><code> Reading and validating wanna yaml config\n\u2139 GCP profile 'default' will be used.\n\u2714 Skipping build for context_dir=pipeline/components/data, dockerfile=pipeline/components/data/Dockerfile and image europe-west1-docker.pkg.dev/cloud-lab-304213/wanna-samples/pipeline_tutorial/data:dev\n\u2838 Compiling pipeline pipeline-tutorial-pipeline\n\u2714 Compiling pipeline pipeline-tutorial-pipeline\n\u2714 Pushing docker image europe-west1-docker.pkg.dev/cloud-lab-304213/wanna-samples/pipeline_tutorial/data:dev\n\u2139 Uploading wanna running manifest to gs://wanna-ml-west1-jh/pipeline-root/pipeline-tutorial-pipeline/deployment/release/dev/wanna_manifest.json\n\u2139 Uploading vertex ai pipeline spec to gs://wanna-ml-west1-jh/pipeline-root/pipeline-tutorial-pipeline/deployment/release/dev/pipeline_spec.json\n\u2714 Pushing pipeline pipeline-tutorial-pipeline\n\u2826 Running pipeline pipeline-tutorial-pipeline in sync modeCreating PipelineJob\n\u280b Running pipeline pipeline-tutorial-pipeline in sync modePipelineJob created. Resource name: projects/968728188698/locations/europe-west1/pipelineJobs/pipeline-pipeline-tutorial-pipeline-20220524103650\nTo use this PipelineJob in another session:\npipeline_job = aiplatform.PipelineJob.get('projects/968728188698/locations/europe-west1/pipelineJobs/pipeline-pipeline-tutorial-pipeline-20220524103650')\nView Pipeline Job: https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/pipeline-pipeline-tutorial-pipeline-20220524103650?project=968728188698\n\u2139 Pipeline dashboard at https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/pipeline-pipeline-tutorial-pipeline-20220524103650?project=968728188698.\nPipelineJob projects/968728188698/locations/europe-west1/pipelineJobs/pipeline-pipeline-tutorial-pipeline-20220524103650 current state: PipelineState.PIPELINE_STATE_RUNNING\n\u2714 Running pipeline pipeline-tutorial-pipeline in sync mode\n</code></pre> <p>You can clearly see the url to the Vertex AI dashboard where you can inspect the pipeline execution, logs, kubeflow inputs and outputs and any logged metadata.</p> <p>You may have noticed in above the line <code>Uploading wanna running manifest to gs://wanna-cloudlab-europe-west1/wanna-pipelines/wanna-sklearn-sample/deployment/dev/manifests/wanna-manifest.json</code> in the logs. This means wanna publishes has its own pipeline manifest which allow us to run any pipeline version with any set of params.</p> <p>Let's try it:</p> <pre><code>echo \"eval_acc_threshold: 0.79\" &gt; pipeline/params.experiment.yaml\n\nwanna pipeline run --manifest gs://wanna-cloudlab-europe-west1/wanna-pipelines/wanna-sklearn-sample/deployment/dev/manifests/wanna-manifest.json --params pipeline/params.experiment.yaml  --sync\n</code></pre> <p>The above snippet will run the pipeline we published earlier with a new set of params. Each manifest version is pushed to <code>gs://${PIPELINE_BUCKET}/pipeline-root/${PIPELINE_NAME}/deployment/release/${VERSION}/wanna_manifest.json</code> so it's easy to trigger these pipelines.</p>"},{"location":"tutorial/profile/","title":"WANNA Profile","text":"<p>We make it easy to deploy your resources to multiple environments (e.g., dev/test/prod) with a simple change in CLI flag or with environment variable change.</p> <p>WANNA Profile is a set of parameters that will cascade down to every instance you want to create unless you overwrite them at the instance level.</p>"},{"location":"tutorial/profile/#loading-wanna-profiles","title":"Loading WANNA Profiles","text":"<p>There are two possible ways to load your profile.</p> <ol> <li>Include the <code>wanna_profiles</code> section in your main WANNA yaml config with an array of profiles</li> <li>Include the <code>wanna_profiles</code> section in separate <code>profiles.yaml</code> saved wherever on your machine and set env variable <code>WANNA_GCP_PROFILE_PATH=/path/to/your/profiles.yaml</code></li> </ol>"},{"location":"tutorial/profile/#selecting-wanna-profile","title":"Selecting WANNA Profile","text":"<p>Now that the CLI knows about your profiles, you need to select the one you want to use. By default, the profile with the name <code>default</code> is used. You can change that with  either <code>WANNA_GCP_PROFILE_NAME=my-profile-name</code> or <code>--profile=my-profile-name</code>.</p> <p>When the selected WANNA Profile is not found, we throw an error.</p>"},{"location":"tutorial/profile/#wanna-profile-parameters","title":"WANNA Profile parameters","text":"class <code>wanna.core.models.gcp_profile.GCPProfileModel</code>(*, profile_name, project_id, zone=None, region, labels=None, bucket, service_account=None, network=None, subnet=None, kms_key=None, docker_repository='wanna', docker_registry=None, env_vars=None) <p><code>wanna_profile</code> section of the yaml config consists of the following inputs:</p> <ul> <li><code>profile_name</code> - [str] name of the WANNA GCP Profile, <code>default</code> will be used if not specified otherwise.   You could use also for example <code>dev</code>, <code>prod</code>, or any other string.</li> <li><code>project_id</code> - [str] GCP project id.</li> <li><code>zone</code> - [str] (optional) GCP location zone.</li> <li><code>region</code> - [str] (optional) GCP location region. If the zone is set and the region not, we automatically   parse the region from the zone (e.g., zone <code>us-east1-c</code> automatically sets region <code>us-east1</code> if the region   is not supplied by the user).</li> <li><code>labels</code> - [dict[str, str]] (optional) GCP resource labels that will be added to all resources you create   with this profile. By default, we also add a few labels based on <code>wanna_project</code> section.</li> <li><code>bucket</code> - [str] (optional) GCS Bucket that can later be used in uploading manifests, storing logs, etc.   depending on the resource type.</li> <li><code>service_account</code> - [str] (optional) GCP service account that will be used by the created resources. If not specified, usually the default service account for each resource type is used.</li> <li><code>network</code> - [str] Google Cloud VPC network name</li> <li><code>subnet</code> - [str] (optional) Google Cloud VPC subnetwork name</li> <li><code>kms_key</code> - [str] (optional) Customer managed enryption key given in format   projects/{project_id}/locations/{region}/keyRings/{key_ring_id}/cryptoKeys/{key_id}   If you get an error, please grant the Service Account with the Cloud KMS CryptoKey Encrypter/Decrypter role</li> <li><code>docker_repository</code> - [str] Wanna Docker Repository</li> <li><code>docker_registry</code> - [str] (optional) Wanna Docker Registry, usually in format {region}-docker.pkg.dev</li> <li><code>env_vars</code> - dict[str, str] (optional) Environment variables to be propagated to all the notebooks and custom jobs</li> </ul>"},{"location":"tutorial/profile/#example-use-case","title":"Example use case","text":"<pre><code>gcp_profiles:\n  - profile_name: default\n    project_id: gcp-test-project\n    zone: europe-west1-b\n    bucket: wanna-ml-test\n    labels:\n      - env: test\n  - profile_name: prod\n    project_id: gcp-prod-project\n    zone: europe-west4-a\n    bucket: wanna-ml-prod\n    labels:\n      - env: prod\n</code></pre> <p>Now the command <code>wanna ...</code> will use the information from the <code>default</code> profile and deploy to  <code>europe-west1-b</code>.</p> <p>When you are ready with your testing, you can call <code>wanna ... --profile=prod</code> to deploy to the production GCP project and zone <code>europe-west4-a</code>.</p>"},{"location":"tutorial/project/","title":"WANNA project","text":"<p>WANNA project settings set some basic values about your project. </p> class <code>wanna.core.models.wanna_project.WannaProjectModel</code>(*, name, version, authors, billing_id=None, organization_id=None) <p><code>wanna_project</code> section of the yaml config consists of the following inputs:</p> <ul> <li><code>name</code> - [str] the name of the wanna project should be unique, this name will be used in the docker service   for naming docker images and in labeling GCP resources. Hence it can be used also for budget monitoring.</li> <li><code>version</code> - [str] Currently used only in labeling GCP resources, we expect to introduce new API versions   and then this parameter will gain more importance.</li> <li><code>authors</code> - [list[str]] Email addresses, currently used only in GCP resource labeling but soon also in monitoring.</li> <li><code>billing_id</code> - [str] (optional) GCP Billing ID, needed for the budget report</li> <li><code>organization_id</code> - [str] (optional) GCP Organization ID, needed for the budget report</li> </ul>"},{"location":"tutorial/project/#example","title":"Example","text":"<pre><code>wanna_project:\n  name: wanna-julia-notebooks\n  version: 1\n  authors: [harry.potter@avast.com, ronald.weasley@avast.com]\n</code></pre>"},{"location":"tutorial/tensorboard/","title":"WANNA Tensorboard","text":"<p>Many GCP services can work directly with Tensorboards. For that reason we offer you a simple solution on how to include them in your resources.</p> <p>Tensorboards can either be used as a separate resource with <code>wanna tensorboard create/list/delete</code> or you can use them similarly to docker images as a dependency of other resources with <code>tensorboard_ref</code> key. In the second case, Tensorboards are automatically created when needed and used when already existing.</p> class <code>wanna.core.models.tensorboard.TensorboardModel</code>(*, name, project_id, zone=None, region, labels=None, description=None, service_account=None, network=None, bucket=None, tags=None, metadata=None) <ul> <li><code>name</code>- [str] Custom name for this instance</li> <li><code>project_id</code> - [str] (optional) Overrides GCP Project ID from the <code>gcp_profile</code> segment</li> <li><code>region</code> - [str] (optional) Overrides region from the <code>gcp_profile</code> segment</li> <li><code>description</code> - [str] (optional) Tensorboard description to be shown in the UI</li> <li><code>labels</code>- [dict[str, str]] (optional) Custom labels to apply to this instance</li> </ul>"},{"location":"tutorial/tensorboard/#example","title":"Example","text":"<pre><code>tensorboards:\n  - name: wanna-sample-dashboard\n\njobs:\n  - name: custom-training-job-with-python-package\n    region: europe-west1\n    worker:\n      python_package:\n        docker_image_ref: tensorflow\n        package_gcs_uri: \"gs://wanna-ml/trainer-0.1.tar.gz\"\n        module_name: \"trainer.task\"\n      args: ['--epochs=100', '--steps=100', '--distribute=single']\n      gpu:\n          accelerator_type: NVIDIA_TESLA_V100\n          count: 1\n    tensorboard_ref: my-nice-tensorboard\n</code></pre>"},{"location":"tutorial/tensorboard/#integration-with-other-services","title":"Integration with other services","text":"<p>Integration with tensorboard depends on the resource. For example Custom Jobs pass the path to the tensorboard in env var <code>AIP_TENSORBOARD_LOG_DIR</code>. When using Keras for training, the integration in your code could look like this:</p> <pre><code>from tensorflow.keras.callbacks import TensorBoard\n\n# Define Tensorboard as a Keras callback\ntensorboard = TensorBoard(\n log_dir=os.getenv(\"AIP_TENSORBOARD_LOG_DIR\"),\n histogram_freq=1,\n write_images=True\n)\nkeras_callbacks = [tensorboard]\n\nmodel.fit(x=train_dataset, epochs=args.epochs, steps_per_epoch=args.steps, callbacks=keras_callbacks)\n</code></pre> <p>Check the job samples for a complete example.</p> <p>With notebooks, you will need <code>tb-gcp-uploader</code> as specified here. We also export the link to the tensorboard directory as <code>AIP_TENSORBOARD_LOG_DIR</code>. But you will need to handle the log export yourself. </p>"},{"location":"tutorial/tensorboard/#roles-and-permissions","title":"Roles and permissions","text":"<p>Permission and suggested roles (applying the principle of least privilege) required for tensorboard manipulation:</p> WANNA action Permissions Suggested Roles create <code>aiplatform.tensorboards.create</code> ,<code>aiplatform.tensorboards.list</code> <code>roles/aiplatform.user</code> delete <code>aiplatform.tensorboards.delete</code> ,<code>aiplatform.tensorboards.list</code> <code>roles/aiplatform.user</code> list <code>aiplatform.tensorboards.list</code>, <code>aiplatform.tensorboardExperiments.*</code>, <code>aiplatform.tensorboardRuns.*</code> <code>roles/aiplatform.viewer</code> , <code>roles/aiplatform.user</code> access the dashboard <code>aiplatform.tensorboards.recordAccess</code> <code>roles/aiplatform.tensorboardWebAppUser</code> <p>Full list of available roles and permission.</p>"}]}